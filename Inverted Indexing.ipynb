{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fd48e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33183698",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"E:\\WIR CS-479\\Assignments 2\\crawled_pages\"\n",
    "\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "inverted_index = defaultdict(lambda: {'frequency': 0, 'posting_list': set()})\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71804f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert string to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words and perform stemming\n",
    "    meaningful_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words and re.match(r'^[a-zA-Z]+$', token)]\n",
    "    # Remove duplicates while preserving order\n",
    "    meaningful_tokens = list(dict.fromkeys(meaningful_tokens))\n",
    "    return meaningful_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f2aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_index(doc_id, tokens):\n",
    "    for token in tokens:\n",
    "        inverted_index[token]['frequency'] += 1\n",
    "        inverted_index[token]['posting_list'].add(doc_id)\n",
    "\n",
    "def write_to_csv(file_path):\n",
    "    with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Keyword', 'Frequency', 'Posting List']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for keyword, info in inverted_index.items():\n",
    "            writer.writerow({'Keyword': keyword, 'Frequency': info['frequency'], 'Posting List': '-'.join(map(str, info['posting_list']))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24899513",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, file_name in enumerate(files):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        # Extract unique ID\n",
    "        doc_id = re.search(r'ID: (\\d+)', content).group(1)\n",
    "        # Extract body text\n",
    "        body_text = re.search(r'Body:\\s+(.*)', content, re.DOTALL).group(1)\n",
    "        # Preprocess text\n",
    "        tokens = preprocess_text(body_text)\n",
    "        # Update inverted index\n",
    "        update_index(doc_id, tokens)\n",
    "\n",
    "# Write inverted index to CSV\n",
    "write_to_csv('inverted_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f4f018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_docs = {}\n",
    "\n",
    "# Read inverted index CSV file to populate the hash table\n",
    "with open('inverted_index.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        keyword = (row['Keyword'])\n",
    "        posting_list = row['Posting List']\n",
    "        # Split posting list by hyphen and store document IDs in a list\n",
    "        doc_ids = posting_list.split('-')\n",
    "        keyword_docs[keyword] = doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97d18b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess keyword\n",
    "def preprocess_keyword(keyword):\n",
    "    # Convert keyword to lowercase\n",
    "    keyword = keyword.lower()\n",
    "    # Remove digits\n",
    "    keyword = re.sub(r'\\d+', '', keyword)\n",
    "    return keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8a7c2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize meta file CSV writer\n",
    "with open('meta_file.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['ID', 'URI', 'Title', 'Description', 'Keywords'])\n",
    "\n",
    "    # Iterate through crawled_pages folder\n",
    "    for filename in os.listdir('crawled_pages'):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join('crawled_pages', filename), 'r', encoding='utf-8') as file:\n",
    "                # Extract ID from the file\n",
    "                file_id = None\n",
    "                uri = None\n",
    "                title = None\n",
    "                body = ''\n",
    "                \n",
    "                # Read lines from the file\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    if line.startswith('ID:'):\n",
    "                        file_id = line[4:].strip()\n",
    "                    elif line.startswith('URL:'):\n",
    "                        uri = line[5:].strip()\n",
    "                    elif line.startswith('Title:'):\n",
    "                        title = line[7:].strip()\n",
    "                    elif line.startswith('Body:'):\n",
    "                        body = ' '.join(lines[lines.index(line)+1:])\n",
    "                        break\n",
    "\n",
    "                # Summary of body - using first two sentences\n",
    "                sentences = re.split(r'[.!?]', body)\n",
    "                description = ' '.join(sentences[:2])\n",
    "\n",
    "                # Find keywords for the webpage\n",
    "                webpage_keywords = []\n",
    "                for keyword, doc_ids in keyword_docs.items():\n",
    "                    if file_id in doc_ids or title.lower().find(preprocess_keyword(keyword)) != -1 or description.lower().find(preprocess_keyword(keyword)) != -1:\n",
    "                        webpage_keywords.append(keyword)\n",
    "\n",
    "                # Write to meta file\n",
    "                writer.writerow([file_id, uri, title, description, ', '.join(webpage_keywords)])\n",
    "\n",
    "print(\"Meta file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68767ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exampl', 'sentenc', 'variou', 'word', 'like', 'run', 'swim', 'jump']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert string to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words and re.match(r'^[a-zA-Z]+$', token)]\n",
    "    # Initialize stemmer and lemmatizer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Apply stemming and lemmatization\n",
    "    meaningful_tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in filtered_tokens]\n",
    "    # Remove duplicates while preserving order\n",
    "    meaningful_tokens = list(dict.fromkeys(meaningful_tokens))\n",
    "    return meaningful_tokens\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is an example sentence with various words like running, swimming, and jumped.\"\n",
    "preprocessed_tokens = preprocess_text(text)\n",
    "print(preprocessed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b4401d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'various', 'word', 'like', 'run', 'swim', 'jump']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from autocorrect import Speller\n",
    "\n",
    "# Initialize spell checker\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "def replace_meaningless_words(tokens):\n",
    "    meaningful_tokens = []\n",
    "    for token in tokens:\n",
    "        if wordnet.synsets(token):\n",
    "            # If token exists in WordNet, consider it meaningful\n",
    "            meaningful_tokens.append(token)\n",
    "        else:\n",
    "            # If token is meaningless, try to find a meaningful alternative\n",
    "            meaningful_alternative = find_meaningful_alternative(token)\n",
    "            meaningful_tokens.append(meaningful_alternative if meaningful_alternative else token)\n",
    "    return meaningful_tokens\n",
    "\n",
    "def find_meaningful_alternative(word):\n",
    "    # Use spell checker to find a meaningful alternative\n",
    "    corrected_word = spell(word)\n",
    "    if corrected_word != word and wordnet.synsets(corrected_word):\n",
    "        return corrected_word\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "preprocessed_tokens = ['exampl', 'sentenc', 'various', 'word', 'like', 'run', 'swim', 'jump']\n",
    "meaningful_tokens = replace_meaningless_words(preprocessed_tokens)\n",
    "print(meaningful_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d39baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from autocorrect import Speller\n",
    "\n",
    "# Initialize spell checker\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "def replace_meaningless_words(tokens):\n",
    "    meaningful_tokens = []\n",
    "    for token in tokens:\n",
    "        if wordnet.synsets(token):\n",
    "            # If token exists in WordNet, consider it meaningful\n",
    "            meaningful_tokens.append(token)\n",
    "        else:\n",
    "            # If token is meaningless, try to find a meaningful alternative\n",
    "            meaningful_alternative = find_meaningful_alternative(token)\n",
    "            meaningful_tokens.append(meaningful_alternative if meaningful_alternative else token)\n",
    "    return meaningful_tokens\n",
    "\n",
    "def find_meaningful_alternative(word):\n",
    "    # Use spell checker to find a meaningful alternative\n",
    "    corrected_word = spell(word)\n",
    "    if corrected_word != word and wordnet.synsets(corrected_word):\n",
    "        return corrected_word\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Read input Excel file\n",
    "input_file = \"Book1.xlsx\"  # Modify this with your file name\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Convert any boolean values to strings in the 'Keywords' column\n",
    "df['Keyword'] = df['Keyword'].astype(str)\n",
    "\n",
    "# Preprocess keywords and replace meaningless words\n",
    "df['Processed Keywords'] = df['Keyword'].apply(lambda x: replace_meaningless_words(str(x).split()))\n",
    "\n",
    "# Write processed keywords to a new Excel file\n",
    "output_file = \"processed_keywords.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"Processed keywords have been written to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26cdc71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
